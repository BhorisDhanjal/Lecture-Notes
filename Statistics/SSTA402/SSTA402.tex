\documentclass[oneside,11pt,pdftex]{book}%Remove draft when book editing is completed.
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{fontawesome5}
\usepackage{booktabs}
\usepackage{amssymb}	
\usepackage{longtable}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,spacing=true,factor=1100,stretch=10,shrink=10]{microtype}
\usepackage[toc,page]{appendix}
\usepackage[nottoc]{tocbibind}
\numberwithin{equation}{section}
\graphicspath{ {./Images/} }
%\usepackage[raggedright]{titlesec}
\usepackage{placeins}
\usepackage{mathtools}


\usepackage{fancyhdr}
\usepackage{hyperref}
%Be careful when you use commands which align formulas.
%If aligned formulas range to two pages, the formulas should be divided into two environments.
%\makeatletter
%\AtBeginDocument{\let\mathaccentV\AMS@mathaccentV}
%\makeatother
%This is a patch for double bar.
%Activate it if \bar{\bar{a}} doesn't work.

\newskip\thskip
\thskip=0.5\baselineskip plus 0.2\baselineskip minus 0.2\baselineskip

\newdimen\dtest%Remove this when book editing is completed.
\settowidth{\dtest}{letters and symbols here}
\typeout{<<<\the\dtest>>>}

\newtheorem{theorem}{Theorem}[chapter]%Modify these declarations for your need.
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}

\newtheorem{xca}[theorem]{Exercise}

\newtheorem{remark}[theorem]{Remark}

\numberwithin{section}{chapter}
\numberwithin{equation}{chapter}

\makeindex

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\F}{\mathbb{F}}



\begin{document}


\frontmatter

\thispagestyle{empty}
\begin{flushright}
{\LARGE \textbf{Bhoris Dhanjal}}%Input your name here.
\end{flushright}
\vfill
\begin{center}
{\fontsize{29.86truept}{0truept}\selectfont \textbf{Analysis of Variance \& Design of Experiments}}%Input the book title here.
%Below is for a book with a subtitle.
%{\fontsize{29.86truept}{0truept}\selectfont \textbf{The Book Title}} \\
%\vspace{6.5truept}
%{\Large, \LARGE, etc. \textbf{The Subtitle}}
\end{center}
\vfill
\begin{flushleft}
{\LARGE \textbf{Lecture Notes}} \\
\hspace{-1.75truept}
{\large \textbf{for SSTA402}}
\end{flushleft}
\newpage

\tableofcontents


\mainmatter

\chapter{Analysis of Variance\\ (Fixed effect models)}
\section{Chi-squared distribution}
\begin{definition}\label{def:chisquaredist}
	Consider the standard normal distribution \[ Z=\frac{x-\mu}{\sigma} \sim \text{SN}(0,1)\]
The square of the standard normal distribution gives us chi-squared with degree of freedom 1.

\par In general for iid SN variates $ Z_i $ we can say
\[ \sum_{i=1}^n Z_i^2 \sim \chi^2_{(n)} \]
\end{definition}

\subsection{Tests to use depending on type of data}
\begin{itemize}
	\item For one categorical feature (is there a difference in the proportion) we will use \textbf{one sample proportion test.} 
	\item When two categorical features we will use \textbf{chi-squared test.}
	\item When one numeric data (is there is difference in the mean) we will use \textbf{t-test.}
	\item when two numeric data we will use the \textbf{t-test for two samples.}
\end{itemize}

\section{Analysis of variance}
Analysis of variance is a powerful statistical tool for test of significance. 
\begin{definition}[Statistical significance]\label{def:significance}
	We have evidence that the result $ v_c $ in the sample also exists in population
\end{definition}

\textbf{Very large sample: }Most results will be statistically significant.\par
\textbf{Very small sample: }Most results will not be statistically significant.

\begin{definition}[p-value]
	Probability value, it indicates, how likely it is that the result occurred by chance alone.
\end{definition}

The test of significance based on t-distribution is adequate only for testing the significance h difference between two sample means.
\par
When we have 3 or more samples (and we need to test if they all come from the same population) we need an alternative procedure.
\par
\begin{example}
	5 fertilizers are applies to four plots of wheat and yield of wheat on each of the plot is given. We may be interested is to find out whether the effect of these fertilizers on the yields is significantly different or not.
\end{example}

The basic purpose of ANOVA is to test the homogeneity of several means.\par
Variation is inherent in nature, the total variation in any set of numerical data is due to a number of causes which may be classified as 
\begin{enumerate}
	\item Assignable causes
	\item Chance causes
\end{enumerate}

The variation due ot assignable causes can be detected and measured, whereas the variation due to chance causes is beyond the control of human hand and cannot be traced separately. 

\begin{definition}[Analysis of variance]
	According to R.A Fischer, ANOVA is the ``separation of variance ascribable to one group of causes from the variance ascribable to other group''.
\end{definition}

\textbf{Assumptions for ANOVA:} ANOVA test is based on the test statistics F (or variance ratio). For the validity of the F-test in ANOVA, the following assumptions are made
\begin{enumerate}
	\item The observations are independent.
	\item Parent population from which observations are taken is normal
	\item Various treatment and environment effects are additive in nature.;
\end{enumerate}

\begin{theorem}[Cochran's theorem]
	Let $ x_1, x_2, \dots, x_n $ denote a random sample from normal population $ N(0, \sigma^2) $. Let the sum of the squares of theese values be written in in the form \[ \sum_{i=1}^n x_i^2 = Q_1+Q_2+\cdots+Q_k \]
	Where $ Q_j $ is a quadratic form in $ x_1, x_2, \dots, x_n$ with rank (degrees of freedom) $ r_j=1,2,\dots,k $ then the random variables $ Q_1, Q_2, \dots, Q_k $ are mutually independent and $ \frac{Q_i}{\sigma^2} $ is $ \chi^2_k $ variable with $ r_j $ degrees of freedom iff $ \sum_{j=1}^k r_j=n $
\end{theorem}

\section{One-way classification}
Let us suppose that $ N $ observations $ y_{ij}, (i=1,2,\dots, k; j=1,2,\dots, n)$ of a random variable $ Y $ are grouped on some basis, into $ k $ classes of sizes $ n_1, n_2, \dots, n_k $ respectively, $ \left(N=\sum_{i=1}^{k} n_i \right) $ as exhibited in below table.
\[
\begin{bmatrix} 
	\text{Class} & \text{Sample observations} & \text{Total} & \text{Mean} \\
	1 & y_{11} \hfill y_{12} \hfill \dots \hfill y_{1n_1} & T_1 & \overline{y_{1.}}\\
	2 & y_{21} \hfill y_{22} \hfill \dots \hfill y_{2n_1} & T_2 & \overline{y_{2.}}\\
	\vdots & \vdots \hfill \vdots \hfill \ddots \hfill \vdots & \vdots & \vdots \\
	k & y_{k1} \hfill y_{k2} \hfill \dots \hfill y_{kn_{k}} & T_k & \overline{y_{k.}}
\end{bmatrix}
\]

The total variation in the observation $ y_{ij} $ can be split into the following two components: 
\begin{enumerate}
	\item The variation between the classes or the variation due to different bases of the classification, commonly known as treatments.
	\item The variation within the classes, i.e. the inherent variation of the random variable within the observation of a class.
\end{enumerate}
The first type of variation is due to assingable cause which can be detected and controlled by human endeavour and second type of varation is due to chance causes, which are beyond control of human hand.
\par 
The main object of analysis of variance technique is to examine if there is significant difference ebtween the claass means in view of the inherent vairaiblity within the seperate classes.
\par 
\begin{example}
	In particular, let us consider the effect of $ k $ different rations on the yield in milk of $ N $ cows (of the same breed and stock) divided into $ k $ classes of sizes $ n_1, n_2, \dots, n_k $ respectively, $ N=\sum_{i=1}^{k} n_i$.
\end{example}

The sources of variation is,
\begin{enumerate}
	\item Effect of treatments (i.e. classes).
	\item Error ($ \varepsilon $) produced by numerical causes
\end{enumerate}

\subsection{Mathematical model}
\begin{align*}
	y_{ij}&=\mu_i + \varepsilon_{ij} && \\
	&= \mu + \alpha_i + \varepsilon_{ij}&&\\
	&= \mu + (\mu_u - \mu) + \varepsilon_{ij} && i=1,\dots,k; j=1,\dots,n_i
\end{align*}
Where, $ \alpha_i=\mu_i-\mu $, effect due to $ i^{th} $ treatment.
 \[  y_{ij}=\mu + \alpha_i +\varepsilon_{ij}\]
$ y_{ij} $ $ j^{th} $ observation receiving $i^{th}$ treatment, $ \mu $ is general mean effect, $ \mu=\sum_{i=1}^{k}\frac{\mu_i n_i}{N} $, $ \alpha_i =$ effect due to $ i^{th} $ level of factor $ i=1,\dots,k $, 
 
 
 \begin{align*}
  \sum_{i=1}^k n_i \alpha_i = \sum_{i} n_i (\mu_i-\mu) &= \sum n_i\mu_i - \sum n_i\mu = 0
 \end{align*}
 $ \varepsilon_{ij}$ is random error or eror component (assume $ \varepsilon_{ij} \sim N(0, \sigma^2) $)

\subsection{Layout of one way ANOVA}
\[
\begin{bmatrix} 
	\text{Factor level} & \text{Observations} & \text{Total} & \text{Mean} \\
	1 & y_{11} \hfill y_{12} \hfill \dots \hfill y_{1n_1} & \sum_{j=1}^{n_1}y_{1j}=y_1. & \frac{y_1.}{n_1}=\overline{y_{1}.}\\
	2 & y_{21} \hfill y_{22} \hfill \dots \hfill y_{2n_1} & \sum_{j=1}^{n_2}y_{2j}=y_2. & \frac{y_2.}{n_2}=\overline{y_{2}.}\\
	\vdots & \vdots \hfill \vdots \hfill \ddots \hfill \vdots & \vdots & \vdots \\
	k & y_{k1} \hfill y_{k2} \hfill \dots \hfill y_{kn_{k}} & \sum_j y_{kj}=y_k. & \frac{y_k .}{n_k}=\overline{y_{k}.}
\end{bmatrix}
\]
$ y_{ij} $ is $ j^{th} $ observation due to $ i^{th} $ level of factor, $ i=1,2,\dots, k; j=1,2,\dots,n_i $

\begin{align*}
	\sum_{i=1}^k n_i&=n_1+n_2+\cdots n_k = N\\
	y_i.&= \sum_{j=1}^{n_i} y_{ij}, \text{ when, } i=1,2,\dots,k \text{ total of $i^{th}$ row}\\
	y..&= \sum_{i=1}^{k} y_i. = G = \text{Grand total}= \sum_{i=1}^k \left[ \sum_{j=1}^{n_i} y_{ij} \right]\\
	x \overline{y_i.}&= \frac{y_i.}{n_i}=\sum_{j=1}^{n_i}\frac{y_{ij}}{n_i}= \text{Mean of obs of $i^{th}$ level of factor}, i=1,2, \dots, k\\
	\overline{y..}&=\frac{1}{N}\sum_i \sum_j y_{ij}=\frac{1}{N} \sum_i n_i \overline{y_i.}=\text{Overall mean}
\end{align*}

\subsection{Assumptions in the model}
\begin{enumerate}
	\item All the observations are independent and $ y_{ij} \sim N(\mu_i, \sigma_e ^2) $
	\item Different effects are additive in nature.
	\item $ \varepsilon_{ij} $ are i.i.d $ N(0, \sigma_o^2) $ i.e. $ E[\varepsilon_{ij}] =0$ and $ V[\varepsilon_{ij}] =0\ \forall i, j$
\end{enumerate}

\subsection{Hypothesis to be tested}
\subsubsection{Null hypothesis}
We want to test the equality of the population means, i.e. the homogeneity of different treatment. Hence, the null hypothesis is given by 
\[ H_0: \mu_1=\mu_2=\cdots=\mu_k=\mu \]
which reduces to
\[ H_0:\alpha_1=\alpha_2=\cdots=\alpha_k=0 \]

\subsubsection{Alternate hypothesis}
At least two of the means ($ \mu_1, \mu_2, \dots, \mu_k $) are different.

\subsection{Estimating the parameters of the model}

The model is \[ y_{ij} =\mu + \alpha_i + \varepsilon_{ij}, i=1,\dots,k; j=1,\dots, n\]
To estimate the parameters we use least squares method
\begin{theorem}
	Estimating $ \mu $
\end{theorem}
\begin{proof}
\begin{align*}
	E &= \sum_{i=1}^{k} \sum_{j=1}^{n_i} \varepsilon_{ij}^2 = \sum_i \sum_j (y_{ij}-\mu - \alpha_i )^2
	\intertext{Differentiate $ E $ w.r.t $ \mu $ and $ \alpha_i $}
	\frac{\partial E}{\partial \mu}&= \frac{\partial}{\partial \mu } \left[ \sum_i \sum_j (y_{ij} -\mu -\alpha_i)^2 \right]\\
	&=2 \sum_i \sum_j (y_{ij}-\mu- \alpha_i)(-1)
	\intertext{Equating with $ 0 $}
	\frac{\partial E}{\partial \mu }= 0 &\implies \sum_{i=1}^k \sum_{j=1}^{n_1}=0\\
	&\implies \sum_i \sum_j y_{ij}-\sum_i \sum_j \mu - \sum_i \sum_j \alpha_i =0\\
	&\implies \sum_i \sum_j y_{ij}- \mu \sum_i \sum_j 1 - \sum_i \sum_j \alpha_i=0\\
	&\implies \sum_i \sum_j y_{ij}- \mu N - 0 =0\\
	&\implies \sum_i \sum_j y_{ij} - N \hat{\mu}=0\\
	&\implies \hat{\mu}=\frac{\sum_i \sum_j y_{ij}}{N}=\overline{y..}=G
\end{align*}

\end{proof}

\begin{theorem}
	Estimating $ \alpha_i $
\end{theorem}
\begin{proof}
	Begin similarly but now different w.r.t $ \alpha_i  $
	\begin{align*}
		\frac{\partial E}{\partial \alpha_i} &= \frac{\partial }{\partial \alpha_i } \left[ \sum_i \sum_j (y_{ij}-\mu - \alpha_i)^2\right]\\
		&= 2 \sum_j (y_{ij}-\mu - \alpha_i)(-1)\\
		&\implies \sum_j (y_{ij} - \mu - \alpha_i )=0\\
		&\implies \sum_j y_{ij} - \sum_j \mu - \sum_j \alpha_i =0\\
		&\implies y_i. - n_i \mu - n_i \hat{\alpha_i}=0\\
		&\implies \hat{\alpha_i}=\frac{y_i.}{n_i}-\hat{\mu}=\overline{y_i.}-\overline{y..}\\
		&\implies \hat{\alpha_i.}= \overline{y_i.}-\overline{y..}
	\end{align*}

Substituting these in the equation 
\begin{align*}
	y_{ij}&= \hat{\mu}+\hat{\alpha_i}+\varepsilon_{ij}\\
	y_{ij}&=\overline{y..}+(\overline{y_i.}+\overline{y..})+(y_{ij}-\overline{y_i.})
\end{align*}
Observation = Grand mean + deviation due to $ i^{th} $ treatment + residual or error.\\
Subtracting $ x \overline{y..} $ from both sides, squaring and summing over $ i, j$.
\begin{align*}
	\sum_i \sum_j (y_{ij}-\overline{y..})^2&= \sum_i \sum_j ((\overline{y_i.}-\overline{y..})+( y_{ij}-\overline{y_i.}))^2\\
	&= \sum_i \sum_j (\overline{y_i.}-\overline{y..})^2+\sum_i \sum_j (y_{ij}-\overline{y_i.})^2\\
	&+2 \sum_i \sum_j (\overline{y_1.}+\overline{y..})(y_{ij}-\overline{y_i.})\\
	\intertext{But second term is zero as it is sum of deviations from their mean}
	\sum_i \sum_j (y_{ij}-\overline{y..})^2&=\sum_i n_i (\overline{y_i.}-\overline{y..})^2+\sum_i \sum_j (y_{ij}-\overline{y_i.})^2
	\intertext{Sum of squares of total variation = variation due to level of factors + variation due to error}
	\intertext{Total sum of squares = sum of squares due to factor + sum of squares due to error}
	\intertext{Total s.s = between s.s. + within s.s.}
\end{align*}
\end{proof}

\subsection{Degrees of freedom}
The total no. of observations is equal to $ N $. The total no of s.s. is computed from $ N $ observations which are subjected to one restriction.
\\
Therefore, total degrees of freedom (d.f.)=$ N-1 $.\\
Degree of freedom due to factors is $ k-1 $. So now we can say degree of freedom for ss due to error is $ n-k=x $

\subsection{Expectation of various sum of squares}
The model is,
\begin{align*}
	y_{ij}&= \mu + \alpha_i + \varepsilon_{ij}\\
	\overline{y_i.}&= \sum_{j=1}^{n_i}\frac{y_{ij}}{n_i}\\
	&= \sum_j \frac{\mu+\alpha_i + \varepsilon_{ij}}{n_i}\\
	&= \frac{n_i \mu + n_i \alpha_i +\varepsilon_{i}.}{n_i}=\mu + \alpha_i +\frac{\varepsilon_{i}.}{n_i}\\
	\overline{y..}&=\sum_i \sum_j \frac{y_{ij}}{N}\\
	&=\frac{N \mu }{N}+\frac{\sum_i n_i \alpha_i}{N}+\frac{\varepsilon..}{N}\\
	\overline{y..}&=\mu+\overline{\varepsilon..}
\end{align*}
Assume $ \varepsilon_{ij}\sim N(0,\sigma^2) $
\begin{align*}
	\overline{\varepsilon_i.}&=\sum_j  \frac{\varepsilon_{ij}}{n_i}\\
	E[\overline{\varepsilon_i.}]&=0\\
	V[\overline{\varepsilon_i.}]&=\frac{\sigma^2}{n_i}
\end{align*}

\subsubsection{Expectation of s.s. due to factor}
\begin{align*}
	E[s.s. due to facor]&=E\left[\sum_i n_i (\overline{y_i}-\overline{y..})^2 \right]\\
	&=E\left[\sum_i n_i \left((\mu+\alpha_i+\overline{\varepsilon_{i}.})-(\mu+\overline{\varepsilon..})\right)^2\right]
	\intertext{Expand this out...}
	&=\sum_i n_i \alpha_i^2 +E\left[\sum_i n_i (\overline{\varepsilon_{i}.}-\overline{\varepsilon..})^2\right]+0\\
	E\left[\sum_i n_i (\overline{e_i.}-\overline{e..})^2\right]&=\sum_i n_i [E[\overline{e_i.}^2]+E[\overline{e..}^2]-2E[\overline{e_i.}-\overline{e..}]]\\
	&=\sum_i n_i \left[\frac{\sigma^2}{n_i}+\frac{\sigma^2}{N}-2E[\overline{e_i.}-\overline{e..}]\right]\\
	&=k\sigma^2+\sigma^2-2\sum_i n_i (\overline{e_i.}-\overline{e..})\\
	E[s.s. due to factors]&=\sum_i n_i \alpha_i^2+k\sigma^2+\sigma ^2-E\left[2\sum_i n_i (\overline{e_i.}-\overline{e..})\right]\\
	E\left[\sum_i n_i (\overline{e_i.}-\overline{e..})\right]&=E\left[ \overline{e_i.}\sum_i n_i (e../n_i) \right]\\
	&=E[\overline{e..}N\overline{e..}]\\
	&=N E[\overline{e..}^2]=\frac{N \sigma^2 }{N}=\sigma^2\\
	E[s.s. due to factors]&=\sum_i n_i \alpha_i^2+k\sigma^2-\sigma ^2\\
	&=\sum_i n_i \alpha_i^2 +(k-1)\sigma^2
\end{align*}
We can now also say,
\[ \text{Mean s.s}=\frac{\text{s.s. due to factor}}{\text{degrees of freedom}} \]
\begin{align*}
	E\left[ \frac{\text{s.s. due to factors}}{k-1} \right]&=\frac{1}{k-1}\sum_i n_i \alpha_i^2 + \sigma^2
\end{align*}
If $ H_0 $ is true $ \forall i $ then, $ E[\text{m.s.s. due to factor}] =\sigma^2$

\subsubsection{Expectation of error s.s.}

\begin{align*}
	E[\text{Error s.s.}]&=E\left[\sum_i \sum_j (y_{ij}-\overline{y_i.})^2\right]\\
	&=E \left[\sum_i \sum_j (y_{ij}-(\mu + \alpha_i + \overline{e_{i}.}))^2\right]
	\intertext{magic}
	&=E[\sum_i \sum_j (e_{ij}-\overline{e_i.})^2]\\
	&=E\left[\sum_i \sum_j (e_{ij}^2)+\sum_i \sum_j (\overline{e_i.}^2)-2 \sum_i \sum_j (e_{ij}-\overline{e_i.})\right]\\
	&= N \sigma^2 + k \sigma^2 -2 \sum_i \sum_j E[e_{ij}-\overline{e_i.}]
	\intertext{Consider now,}
	\sum \sum E[e_{ij}-\overline{e_i.}]&=\sum_i n_i E[\overline{e_i.}^2]=\sum_i \frac{n_i \sigma^2}{n_i}=k\sigma\\
	E[\text{error s.s.}]&=N\sigma^2+k\sigma^2-2k\sigma^2\\
	&=(N-k)\sigma^2
\end{align*}
Now we can say,
\[ E[\text{m.s.s. for error}]=\sigma^2 \]

From these two subsections we can say $E[\text{m.s.s for factor}]\geq E[\text{m.s.s
 for error}]$. Equality holds when $ H_0 $ is true.

\subsection{Test statistics}
Using Cochran's theorem.\\
We have $ \frac{s.s due to factor}{\sigma^2}, \frac{ss due to error}{sigma^2} $ follows chi squared with $(k-1)$ and $(N-k)$ degrees of freedom respectively.\\
Therefore, the test statistics are defined as follows
\begin{align*}
	f&=\frac{\chi_1^2/d.f.}{\chi_2^2/d.f.}=\frac{\text{s.s. due to factors}/\sigma^2/(k-1)}{\text{s.s. due ot error}/\sigma^2/(N-k)}\\
	&\frac{\text{M.s.s for factor}}{\text{m.s.s. for error}}
\end{align*}
We will reject $ H_0: \alpha_i=0 \forall i $ if $ F_{cal}>F_{tab} $ as two independent $ \chi^2 $ variates with $ (k-1) , (N-k)$ d.f. respectively.

\subsection{Computations forms of s.s.}

\subsubsection{Computational form of total s.s.}
\begin{align*}
	\text{Total s.s.}&= \sum_i \sum_j (y_{ij}-\overline{y..})^2\\
	&=\sum \sum (y_{ij}^2+\overline{y..}^2-2y_{ij}\overline{y..})\\
	&= \sum \sum y_{ij}^2 + N \overline{y..}^2- 2\overline{y..}\sum \sum y_{ij}\\
	&=\sum \sum y_{ij}^2 - \frac{N y^2..}{N^2}\\
	\text{Total s.s.}&= \sum_i \sum_j y_{ij}^2 -\frac{y^2..}{N}=\sum_i \sum_j y_{ij}^2 -\frac{G^2}{N}\\
	\text{Total s.s.}&= \text{Raw s.s.}-G.F
\end{align*}

\subsubsection{Computational form of s.s. due to factors}
\begin{align*}
	\text{s.s. due to factor}&= \sum_i n_i (\overline{y_i}-\overline{y..})^2
	\intertext{Magic...}
	\text{s.s. due to factor}&= \sum_i \frac{T_i}{n_i}-\frac{G^2}{N}
\end{align*}

\subsection{One way ANOVA Table}
Replace R with T below here,

 \[ \begin{bmatrix}
 	\text{Source of variance} & d.f. & s.s. & m.s.s & F\\
 	\text{Between factor (treatment)} & k-1 & \sum_i \frac{R_i^2}{n_i}-\frac{G^2}{N}=S_R^2 & s^2_R=\frac{S_R^2}{k-1} & F=\frac{s_R^2}{s^2_E}\\
 	\text{Within factor (eror)} & N-k & + = S_E^2 & s_E^2=\frac{s_E^2}{N-k} & -\\
 	\text{Total} & N-1 & \sum_i \sum_j y_{ij}^2-\frac{G^2}{N} & - & -
 \end{bmatrix} \]

\subsection{Example of One-way classification ANOVA}
Type this out later 

\section{Two-way classification}
Suppose $ n $ observations are classification into $ k $ categories (or classes), say $ A_1, A_2, \dots, A_k $ according to some criterion $ A $; and into $ h $ categories, say $ B_1, B_2, \dots, B_h $ according to some criterion $ B$ having $ kh $ combinations ($ A_i,B_j $) $ i=1,2,\dots k; j=1,2,\dots, h $; often called cells.\par
This scheme of classification according to two factors or criteria is called two-way classification and its analysis is called two way ANOVA. The number of observations in each cell may be equal or different, but we shall consider the case of one observation per cell so that $ n=kh $, i.e. the total no of cell is $ n=kh $.

\begin{example}
	If we have $ 4 $ different fertilizers say $ A,B,C,D $ and $ 5 $ different types of seeds then we will have $ 20 $ plots with each of oe having one of the four fertilizers and one of the five seeds. The yields from these plots will be analysed to check whether there is significant difference between fertilizers or between seeds there are two was in which we can compare the data the analysis is called two-way analysis.
\end{example}

\subsection{Layout of two-way classification}
\[ \begin{matrix}
	\text{Levels} & 1 & 2 & \dots & j & \dots & h & \text{Total} & \text{Mean}\\
	1 & y_{11} & y_{12} & \dots & y_{1j} & \dots & y_{1h} & y_1. & \overline{y_1.}\\
	2 & y_{21} & y_{22} & \dots & y_{2j} & \dots & y_{2h} & y_2. & \overline{y_2.}\\
	\vdots & \vdots & \vdots & \ddots & \vdots & \ddots & \vdots & \vdots & \vdots\\
	i & y_{i1} & y_{i2} & \dots & y_{ij} & \dots & y_{ih} & y_i. & \overline{y_i.}\\
	\vdots & \vdots & \vdots & \ddots & \vdots & \ddots & \vdots & \vdots & \vdots\\
	k & y_{k1} & y_{k2} & \dots & y_{kj} & \dots & y_{kh} & y_k. & \overline{y_k.}\\
	\text{Total} & y._1 & y._2 & \dots & y._j & \dots & y._h &  & \\
	\text{Mean} & \overline{y._1} & \overline{y._2} & \dots & \overline{y._j} & \dots & \overline{y._h} &  & \\
\end{matrix} \]

\subsection{Notation}
\begin{align*}
	N&=kh\\
	y..&=\sum_i \sum_j y_{ij}=G\\
	\overline{y..}&=\sum_i \sum_j \frac{y_{ij}}{N}=\frac{y..}{kh}\\
\end{align*}
$y_{ij}=\text{The observation receiving $ i^{th} $ level of factor A and $ j^{th} $ level of factor B}$
\begin{align*}
	R_i=y_i.&=\sum_{j=1}^{h}y_{ij}\\
	\overline{y_i.}&=\frac{y_i.}{h}\\
	C_j=y._j&=\sum_{i=1}^k y_{ij}\\
	\overline{y._j}&=\frac{y._j}{k}
\end{align*}

\subsection{Assumptions of two-way classification}
\begin{enumerate}
	\item Observations are independent.
	\item Different effects are additive in nature
	\item $ e_{ij} $ are i.i.d $ N(0,\sigma^2) $
\end{enumerate}

\subsection{Model of two-way classification}
\[ y_{ij} = \mu_{ij}+e_{ij}, i=1,\dots,k; j=1,\dots,h\]
Here, $ e_{ij} $ is effect due to chance causes. $ E[y_{ij}] =\mu_{ij}=$fixed effect due to assingable causes where $ y_{ij} $ are indepdent $ N(\mu_{ij},\sigma^2) $. \\
$ \mu_{ij} $ is further split into the followring parts 
\begin{enumerate}
	\item The general mean effect is given by \[ \mu=\sum_i \sum_j \frac{\mu_{ij}}{N} \]
	\item $ \alpha_i  $ effect due to $ i^{th} $ level of factor $ A $, $ \alpha_i=\mu_i-\mu, i=1,2,\dots, k$ and $ \mu_i.=\sum_j \frac{\mu_{ij}}{h}, i=1,2,\dots, k $. Sum of $ \alpha_i  $ is zero.
	\begin{proof}
		\begin{align*}
			\sum_{i=1}^k \alpha_i &= \sum_i (\mu_i.-\mu)\\
			&=\sum_i  \mu_i.-\mu k\\
			&=k\sum_i \sum_j \frac{\mu_{ij}}{hk}-\mu k =0
		\end{align*}
	\end{proof}
	\item $ \beta_j $ effect due to $ j^{th} $ level of factor $ B $. $ \beta_j=\mu._j-\mu; j=1,\dots,h $ and same thing as above write it out.
	\item The interaction effect $ \gamma_{ij} $ when the $ i^{th} $ level of first factor and $ j^{th} $ level of factor $ B $ occurs simultaneously and is given by 
	\[ \gamma_{ij}= \mu_{ij}-\mu_i.-\mu._j-\mu\]
	And its summation across j and i is zero.
\end{enumerate}
Thus, we have $ \mu_{ij} =\mu+(\mu_i.-\mu)+(\mu._j-\mu)+(\mu_{ij}-\mu_i.-\mu._j-\mu)$

As there is only one observation per cell we cannot estimate interactive effect. Hence interaction effect is zero and the model reduces to.
\[ y_{ij}=\mu+\alpha_i+\beta_j+e_{ij} \]
for $ i=1,\dots,k;j=1,\dots,h $

\subsection{Hypothesis}
\subsubsection{Null hypothesis}
$ H_{0_{\alpha}}: $ There is no significant difference of factor A,
\begin{align*}
	\mu_1.=\mu_2.=\cdots=\mu_k.=\mu\\
	\alpha_1=\alpha_2=\cdots=\alpha_k=0
\end{align*}
$ H_{0_{\beta}}: $ There is no significant difference of factor B,
\begin{align*}
	\mu._1=\mu._2=\cdots=\mu._k=\mu\\
	\beta_1=\beta_2=\cdots=\beta_h=0
\end{align*}

\subsubsection{Alternative hypothesis}
$ H_{1 \alpha } :$ At least two of the $ \mu_i. $'s are different.\\
i.e. At least one of the $ \alpha_i  $'s is not zero.\\
$ H_{1 \beta } :$ At least two of the $ \mu._j $'s are different.\\
i.e. At least one of the $ \beta_j  $'s is not zero.

\subsection{Least square estimators of parameters}
To obtain $ \hat{\mu}, \hat{\alpha_i} , \hat{\beta_j}$
\begin{align*}
	E&=\sum_i \sum_j e_{ij}^2=\sum_i \sum_j (y_{ij}-\mu-\alpha_i -\beta_j)^2
\end{align*}
First derivate it with respect to $ \mu  $ and equate it to zero to get $ \hat{\mu } $
\begin{align*}
	\frac{\partial E}{\partial \mu }=0 &\implies 2 \sum_i \sum_j (y_{ij}-\mu-\alpha_i - \beta_j)(-1)=0\\
	&\implies \sum_i \sum_j y_{ij} - \sum_i \sum_j \mu - \sum_i \sum_j \alpha_i - \sum_i \sum_j \beta_j=0\\
	&\implies y..-\mu N - 0 - 0\\
	&\implies \hat{\mu}=\frac{y..}{N}=\overline{y..}
\end{align*}

Derivative it with respect to $ \alpha_i $
\begin{align*}
	\frac{\partial E}{\partial \alpha_i }=0 &\implies 2 \sum_j (y_{ij}-\mu-\alpha_i - \beta_j)(-1)=0\\
	&\implies \sum_j y_{ij} - \sum_j \mu - \sum_j \alpha_i - \sum_j \beta_j=0\\
	&\implies \sum_j y_{ij} - h \mu - \sum_j \alpha_i - \sum_j \beta_j =0
	\intertext{$ \sum_j \beta_j=0 $ so,}
	&\implies \sum_j y_{ij} - h \widehat{\mu} = h \widehat{\alpha_i}\\
	&\implies \widehat{\alpha_i}=\frac{1}{h}\sum_j y_{ij}-\widehat{\mu}\\
	&\implies \widehat{\alpha_i}=\overline{y_i.}-\overline{y..}
\end{align*}

Differentiate it with respect to $ \beta_j $
\begin{align*}
		\frac{\partial E}{\partial \beta_j }=0 &\implies 2 \sum_i (y_{ij}-\mu-\alpha_i - \beta_j)(-1)=0\\
		&\implies \sum_i y_{ij} - \sum_i \mu - \sum_i \alpha_i - \sum_i \beta_j=0
		\intertext{Similar steps as above}
		&\implies \hat{\beta_j}=\overline{y._j}-\overline{y..}
\end{align*}

Now we can find the value of $ e_{ij} $,
\begin{align*}
	y_{ij}&=\mu+\alpha_i+\beta_j+e_{ij}\\
	&=\overline{y..}+\overline{y_i.}-\overline{y..}+\overline{y._j}-\overline{y..}+e_{ij}\\
	&=\overline{y_i.}+\overline{y._j}-\overline{y..}+e_{ij}\\
	\implies e_{ij}&=y_{ij}-\overline{y_i.}-\overline{y._j}+\overline{y..}
\end{align*}

The model is then,
\begin{align*}
	y_{ij}&=\overline{y..}+(\overline{y_i.}-\overline{y..})+(\overline{y._j}-\overline{y..})+(y_{ij}-\overline{y_i.}-\overline{y._j}-\overline{y..})
	\intertext{Subtracting $ \overline{y..} $ from both the sides squaring both sides and summing over $ i $ and $ j $.}
	\sum_i \sum_j (y_{ij}-\overline{y..})^2&=\sum_i \sum_j \left[(\overline{y_i.}-\overline{y..})+(\overline{y._j}-\overline{y..})+(y_{ij}-\overline{y_i.}-\overline{y._j}-\overline{y..}) \right]^2\\
	&=\sum_i \sum_j (\overline{y_i}-\overline{y..})^2+\sum_i \sum_j (\overline{y._j}-\overline{y..})^2+\sum_i \sum_j (y_{ij}-\overline{y_i.}-\overline{y._j}-\overline{y..})^2
	\intertext{All cross product terms vanish}
	\text{Total s.s.}&=\text{Factor a.s.s+Factor b.s.s.+Error s.s.}
\end{align*}

\subsection{Degrees of freedom}
\begin{itemize}
	\item Total d.f. $ N-1 $.
	\item Factor A d.f. $k-1$.
	\item Factor B d.f. $h-1$
	\item Error d.f. $N-k-h+1=(k-1)(h-1)$
\end{itemize}

\subsection{Mean sum square}
\begin{itemize}
	\item $ MSS_A = \frac{SS_A}{k-1}$
	\item $ MSS_B = \frac{SS_B}{h-1}$
	\item $MSS_{err}=\frac{Err SS}{(k-1)(h-1)}$
\end{itemize}

\subsection{Expectation of various SS}
The model is \[ y_{ij}=\mu+ \alpha_i+\beta_j+e_{ij} \]
for $ i=1,\dots,k; j=1,\dots h $

\begin{align*}
	\overline{y_i.}&=\mu+\alpha_i+\overline{e_i.}
\end{align*}
Similarly
\begin{align*}
	\overline{y._j}&=\mu+\beta_j+\overline{e._j}
\end{align*}
\begin{align*}
	\overline{y..}=\mu + \overline{e..}
\end{align*}

We assume the following,
\begin{align*}
	e_{ij}\sim N(0, \sigma^2)\\
	\overline{e_i.}\sim N \left( 0, \frac{\sigma^2}{h} \right)\\
	\overline{e._j}\sim N \left( 0, \frac{\sigma^2}{k} \right)\\
	\overline{e..}\sim N\left(0, \frac{\sigma^2}{N}\right)
\end{align*}
We now find expectation of error ss.
\begin{align*}
	E[SS_{err}]&=E\left[\sum_i \sum_j (y_{ij}+\overline{y_i.}+\overline{y._j}+\overline{y..})^2\right]\\
	&=E\left[\sum_i \sum_j \left( y_{ij}- (\mu + \alpha_i + \overline{e_i.})-(\mu + \beta_j - \overline{e.j})+(\mu +\overline{e..}) \right)^2\right]\\
	&=E\left[\sum \sum \left(e_{ij}+\overline{e_i.}+\overline{e._j}+\overline{e..}\right)^2\right]
	\intertext{All the product terms are zero,}
	&=E[\sum_i \sum_j e{ij}^2-h \sum_i \overline{e_i.}^2-k\sum_j \overline{e._j}^2+N \overline{e..}^2]\\
	&=N\sigma^2-hk\frac{\sigma^2}{k}-kh \frac{\sigma^2}{h}+N\frac{\sigma^2}{N}\\
	&=(N-h-k+1)\sigma^2\\
	&=(kh-h-k+1)\sigma^2\\
	E[SS_{err}]&=(h-1)(k-1)\sigma^2
\end{align*}
So we also have then 
\begin{align*}
	E\left[\frac{SS_{err}}{(k-1)(h-1)}\right]&=\sigma^2\\
	E[MSSE]&=\sigma^2
\end{align*}
Therefore MSSE is unbiased estimator of $ \sigma^2 $.\\
Now we will find expectation of ss due to factor $ A $,
\begin{align*}
	E[SS_A]&=E\left[\sum_i h \left(\overline{y_i.}-\overline{y..}\right)^2\right]\\
	&=E \left[\sum_i h \left( \mu + \alpha_i + \overline{e_i.} - \mu + \overline{e..} \right)^2\right]\\
	&= E \left[\sum_i h \left( \alpha_i +\overline{e_i.}+\overline{e..} \right)^2\right]\\
	&= E \left[\sum_i h \left(\alpha_i^2 + (\overline{e_i.}+\overline{e..})^2+2\alpha_i (\overline{e_i.}+\overline{e..})\right)\right]\\
	&=E \left[\sum_i h \alpha_i^2 + h \sum_i (\overline{e_i.}-\overline{e..})^2 + 2 \sum_i \alpha_i (\overline{e_i.}-\overline{e..})\right]\\
	&= \sum_i h \alpha_i^2 + E\left[h \sum_i (\overline{e_i.}-\overline{e..})^2\right]+2 \sum_i \alpha_i E[\overline{e_i.}-\overline{e..}]\\
	&= h \sum_i \alpha_i^2 + E\left[h \left(\sum_i \overline{e_i}^2+\sum_i \overline{e..}^2- 2\sum_i \overline{e_i.}\overline{e..}\right)\right]\\
	&= h \sum_i \alpha_i^2 + h \sum_i E[\overline{e_i.}^2]+hk E[\overline{e..}^2]-2kh E[\overline{e..}^2]\\
	&= h \sum_i \alpha_i^2 + kh \frac{\sigma^2}{h}+hk \frac{\sigma^2}{N}-2N \frac{\sigma^2}{N}\\
	&= h \sum_i \alpha_i^2 + (k-1)\sigma^2
\end{align*}
Therefore we have $ E[MSS_A]=\frac{h}{k-1}\sum_{i}\alpha_i^2+\sigma^2 $.\\
So this will be a unbiased estimator of $ \sigma^2 $ when $ H_{0_\alpha} $ is true, i.e. $ \sum \alpha_i^2=0 $.

Now we will find expectation of ss due to factor B in the exact same manner,
\begin{align*}
	E[SS_B]&=E\left[\sum_j k \left(\overline{y_i.}-\overline{y..}\right)^2\right]\\
	&=E \left[\sum_j k \left( \mu + \beta_j + \overline{e_.j} - \mu + \overline{e..} \right)^2\right]\\
	&= E \left[\sum_j k \left( \beta_j +\overline{e._j}+\overline{e..} \right)^2\right]\\
	&= E \left[\sum_j k \left(\beta_j^2 + (\overline{e._j}+\overline{e..})^2+2\beta_j (\overline{e._j}+\overline{e..})\right)\right]\\
	&=E \left[\sum_j k \beta_j^2 + k \sum_j (\overline{e._j}-\overline{e..})^2 + 2 \sum_j \beta_j (\overline{e._j}-\overline{e..})\right]
	\intertext{I can't be bothered to type this out again its the exact same f thing just do it.}
%	&= \sum_i h \alpha_i^2 + E\left[h \sum_i (\overline{e_i.}-\overline{e..})^2\right]+2 \sum_i \alpha_i E[\overline{e_i.}-\overline{e..}]\\
%	&= h \sum_i \alpha_i^2 + E\left[h \left(\sum_i \overline{e_i}^2+\sum_i \overline{e..}^2- 2\sum_i \overline{e_i.}\overline{e..}\right)\right]\\
%	&= h \sum_i \alpha_i^2 + h \sum_i E[\overline{e_i.}^2]+hk E[\overline{e..}^2]-2kh E[\overline{e..}^2]\\
%	&= h \sum_i \alpha_i^2 + kh \frac{\sigma^2}{h}+hk \frac{\sigma^2}{N}-2N \frac{\sigma^2}{N}\\
	&= k \sum_j \beta_j^2 + (k-1)\sigma^2
\end{align*}
So now we have $ E[MSS_B]=\frac{k}{h-1}\sum_j \beta_j^2+\sigma^2 $. So we have that $ MSS_B $ is an unbiased estimator of $ \sigma^2 $ when $ H_{0_\beta} $ is true i.e. $ \sum_j \beta_j^2=0. $

\subsection{Test statistics}

Total s.s. = s.s. due to factor A + s.s. due to factor B + SSE
$ N-1=(k-1)+(h-1)+(k-1)(h-1) $\\

Therefore by Cochran's theorem,
\begin{align*}
	\chi_1^2&=\frac{SS_A}{\sigma^2}\sim \chi^2_{k-1}\\
	\chi_2^2&=\frac{SS_B}{\sigma^2} \sim \chi^2_{h-1}\\
	\chi_3^2&=\frac{SS_{err}}{\sigma^2} \sim \chi^2_{(k-1)(h-1)}
\end{align*}
And we have the test statistics as follows,
\begin{align*}
	F_1&=\frac{\chi_1^2/(k-1)}{\chi^2_3/(h-1)(k-1)}\\
	F_2&=\frac{\chi_2^2/(h-1)}{\chi_3^2/(h-1)(k-1)}
\end{align*}

Also $ E[MSS_A]>E[MSSE] $ then $ H_{0_1} $ is rejected.\\
$ E[MSS_B]>E[MSSE] $ then $ H_{0_2} $ is rejected.

\subsection{Two way ANOVA Table}
Replace R with T below here,

\[ \begin{bmatrix}
	\text{Source of variance} & d.f. & S.S. & S.S.S & F_{cal}\\
	\text{Between factor A} & k-1 & RSS=\sum_i^k \frac{R_i^2}{n_i}-\frac{G^2}{N} & MRSS=\frac{RSS}{k-1} & F_1=\frac{MRSS}{MESS}\\
	\text{Between factor B} & h-1 & CSS=\sum_j^h \frac{R_j^2}{n_j}-\frac{G^2}{N} & MCSS=\frac{CSS}{h-1} & F_2=\frac{MLSS}{MESS}\\
	\text{Error} & (k-1)(h-1) & ESS=+ & MESS=\frac{CSS}{(h-1)(k-1)} & -\\
	\text{Total} & N-1 & T_{ss}=\sum_i \sum_j y_{ij}^2-\frac{G^2}{N} & - & -
\end{bmatrix} \]

\section{Critical difference (CD)}
\begin{itemize}
	\item When we reject $ H_0 $ we can conclude that the treatment mean are not all equal, but we cannot be more sure than this.
	\item We do not know whether all the means are different from one another or only some of them are different.
	\item Therefore, once we reject the null hypothesis, we conduct further tests to find out which of the means are actually different.
	\item This is achieved by comparison of means with the critical difference value.
\end{itemize}

\subsection{Aim of Critical difference}
\begin{itemize}
	\item To determine the difference between a pair of means that will be significant.
	\item To compare that value with teh calculated differences between all pairs of group means.
\end{itemize}
If the difference between two means is greater than the critical difference it can be concluded that the difference between this pair of means is significant.

\begin{itemize}
	\item In ANOVA, the null and alternative hypothesis are ....
	\item When $ H_0 $ is false then we have to find out those differences between the means which are significant.
	\item For this we calculate standard error of difference between means
\end{itemize}

\subsection{Standard error of difference}
\begin{definition}
	\[ SE_d=\sqrt{EMS\left(\frac{1}{n_1}+\frac{1}{n_2}\right)} \]
\end{definition}
\begin{itemize}
	\item $ n_1 , n_2$ are the number of observations based on the number of replications of different means under comparisons, if $ n_1=n_2$ then 
	\[ SE_d=\sqrt{\frac{2 EMS}{r}} \]
	\item $ r= $number of replications which is equal for all treatments under comparison
	\item The critical difference at desired level of significant is calculated and the observed difference are very easily compared with CD-value.
\end{itemize}
The CD value is calculated as follows,
\[ CD=SE_d \cdot T_{tab} \]
Use t-table.
\textbf{Least significant difference (LSD):} CD value at $ 5 \% $ LOS.


\begin{example}
	Five variates of feeds were equally and randomly allotted to 20 pigs of same sex and approximately of the same body weight. The gain body weight recorded in the feeding experiment were as given in the Table. Analyse data for comparison of feeds.
	\begin{table}[!htp]
		\centering
		\begin{tabular}{@{}ccccccc@{}}
			\toprule
			\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Source of\\ Variation\end{tabular}} & \multirow{2}{*}{d.f.} & \multirow{2}{*}{s.s.} & \multirow{2}{*}{m.s.s.} & \multirow{2}{*}{$F_{cal}$} & \multicolumn{2}{c}{$F_{tab}$} \\ \cmidrule(l){6-7} 
			&  &  &  &  & \multicolumn{1}{l}{$5 \%$} & \multicolumn{1}{l}{$1 \%$} \\ \midrule
			Between feeds & 4 & 155.2 & 38.8 & 11.18 & 3.06 & 4.89 \\
			Within feeds & 15 & 52.0 & 3.47 &  &  &  \\
			Total & 19 & 207.2 &  &  &  &  \\ \bottomrule
		\end{tabular}
	\end{table}
	\FloatBarrier
\end{example}
\begin{proof}
	\begin{itemize}
		\item However, the F-test is significant even if any of two means differ.
		\item Thus, when a significant effect is found using ANOVA, we don't know which means actually differ significantly from each other.
		\item Compute $ SE_d=\frac{2 EMS}{r} = \sqrt{\frac{2(3.47)}{4}}=1.317$. And we get $ CD=2.80 $
		\item For comparison of means under different variatees of feeds, we arrange means in descending order of magnitude as follows.
		\begin{itemize}
			\item $ F_3=16 $
			\item $ F_4=12 $
			\item $ F_2=11 $
			\item $ F_5=9 $
			\item $ F_1=8 $
		\end{itemize}
		\item Compute the differences now and we get that $ (F_4,F_3),(F_3,F_2),(F_3,F_5),(F_3,F_1), \cdots $ have significant differences, if difference is greater than CD.
		\item To make this easily interpretable we add same superscript over all those means which do not differ from each other in the following way
	\end{itemize}
	In our example we could see the following conclusions
	\begin{itemize}
		\item $ F_3 $ gave the maximum weight gain and was significantly different from all other means.
		\item $ F_4, F_2 $ did not differ from each other.
		\item Similarly, there were no significant difference between feed variates $ F_2, F_5 $ are also between $ F_5, F_1. $
		\item Therefore, $ F_3 $ is a recommended feed.
	\end{itemize}
\end{proof}



\chapter{Design of Experiments}
Ratio of design $ D_1 $ with $ D_2 $ are the ratio of their error variance $ E=\frac{\sigma_2^2}{\sigma_1^2} \frac{n_1}{n_2}$

\section{Completely randomized design}
\subsection{Statistical analysis}
$ y_{ij} $ denotes response measurement of $ j^{th} $ experiment unit receiving $ i^{th} $ treatment for $ i=1,2,\dots,k $ and $ j=1,2,\dots,n $.

Mathematical model is the same as that of ANOVA one way classification.

$ y_{ij} \sim N(\mu+\alpha_i, \sigma^2) $
$ \mu=\frac{\sum_i^k n_i \mu_i}{\sum n_i} $
TSS=SST+SSE
$ TSS=\sum (y_{ij}-\overline{y..})^2 $ with $ N-1 $d.f.
$ SST= \sum n_i(y) $
\textbf{complete this up later its same as anova }
\subsection{Estimation of treatment constants}
Let $ c_1,c_2,\dots,c_k $ be constants such that their sum is zero then the linear combination $ \sum_i c_i \alpha_i $ is called treatment constant.

An estimate of $ \sum_i c_i \alpha_i $ is $ \sum_i c_i \overline{y_i.} $, 

So we have \[ V(\sum_i c_i \hat{\alpha_i})=\sum c_i^2 \frac{\sigma^2}{n_i} \]

So estimate of standard error of $ \sum c_i \hat{\alpha_i} $ is 
\begin{align*}
	\sqrt{MSE \sum_i \frac{c_i^2}{n_i}}
\end{align*}

So the confidence intervals is 
\[ \left[ \sum_i c_i \overline{y_i.}-t_{(N-k,\alpha/2)}\sqrt{MSSE \sum \frac{c_i^2}{n_i}},\overline{y_i.}+t_{(N-k,\alpha/2)}\sqrt{MSSE \sum \frac{c_i^2}{n_i}} \right]\]

\subsection{Advantages, disavantages and applications}
\subsubsection{Advantages}
\begin{enumerate}
	\item It is easy to layout the design
	\item All experimental units can be used, i.e. it results in maximum use of experimental material.
	\item Complete flexibility is allowed, any number of treatments and replicates may be used.
	\item Relatively easy statistical analysis, even with variable replicates and variable experimental errors for different treatments.
	\item Analysis remains simple when data are missing.
	\item Provides the maximum number of degrees of freedom for error for a given number of experimental units and treatments.
\end{enumerate}
\subsubsection{Disadvantages}
\begin{enumerate}
	\item Relatively low accuracy due to lack of restrictions which allows environmental variation to enter experimental error.
	\item Not suited for large number of treatments because a relatively large amount of experimental material is needed which increases the variation.
\end{enumerate}
\subsubsection{Applications}
\begin{enumerate}
	\item Under conditions where the experimental material is homogenous, i.e. laboratory, or green house experiments or growth chamber experiments.
	\item Where a fraction of the experimental units is likely to be destroyed or fail to respond.
	\item In small experiments where there is a small number of degrees of freedom.
\end{enumerate}

\section{Randomized block designs}






\chapter{Latin square design (LSD)}


\backmatter
\thispagestyle{empty}
\newpage

\end{document}
