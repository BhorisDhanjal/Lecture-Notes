%This template consists of the minimum of a single book.
%Please do not think this template is mandatory and the format must be followed strictly.
%We expect the author adds what he needs.
\documentclass[oneside,11pt,pdftex]{book}%Remove draft when book editing is completed.
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{fontawesome5}
\usepackage{booktabs}
\usepackage{amssymb}	
\usepackage{longtable}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,spacing=true,factor=1100,stretch=10,shrink=10]{microtype}
\usepackage[toc,page]{appendix}
\usepackage[nottoc]{tocbibind}
\numberwithin{equation}{section}
\graphicspath{ {./Images/} }
%\usepackage[raggedright]{titlesec}
\usepackage{placeins}
\usepackage{mathtools}


\usepackage{fancyhdr}
\usepackage{hyperref}
%Be careful when you use commands which align formulas.
%If aligned formulas range to two pages, the formulas should be divided into two environments.
%\makeatletter
%\AtBeginDocument{\let\mathaccentV\AMS@mathaccentV}
%\makeatother
%This is a patch for double bar.
%Activate it if \bar{\bar{a}} doesn't work.

\newskip\thskip
\thskip=0.5\baselineskip plus 0.2\baselineskip minus 0.2\baselineskip

\newdimen\dtest%Remove this when book editing is completed.
\settowidth{\dtest}{letters and symbols here}
\typeout{<<<\the\dtest>>>}

\newtheorem{theorem}{Theorem}[chapter]%Modify these declarations for your need.
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}

\newtheorem{xca}[theorem]{Exercise}

\newtheorem{remark}[theorem]{Remark}

\numberwithin{section}{chapter}
\numberwithin{equation}{chapter}

\makeindex

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\F}{\mathbb{F}}

\graphicspath{ {graphics/} }

\begin{document}


\frontmatter

\thispagestyle{empty}
\begin{flushright}
{\LARGE \textbf{Bhoris Dhanjal}}%Input your name here.
\end{flushright}
\vfill
\begin{center}
{\fontsize{29.86truept}{0truept}\selectfont \textbf{Probability and Sampling Distributions (B)}}%Input the book title here.
%Below is for a book with a subtitle.
%{\fontsize{29.86truept}{0truept}\selectfont \textbf{The Book Title}} \\
%\vspace{6.5truept}
%{\Large, \LARGE, etc. \textbf{The Subtitle}}
\end{center}
\vfill
\begin{flushleft}
{\LARGE \textbf{Lecture Notes}} \\
\hspace{-1.75truept}
{\large \textbf{for SSTA401}}
\end{flushleft}
\newpage

\tableofcontents


\mainmatter

\chapter[Continuous probability distributions]{Transformation of random variables \& standard univariate continuous probability distributions}
\section{Uniform/Rectangular distributions}
\begin{definition}\label{def:uniformdist}
	A r.v. $ X$ is said to follow uniform distribution over an interval $ (a,b) $ if its pdf is constant over the entire range.
\end{definition}
\subsection{PDF of uniform distribution}
\begin{theorem}
	PDF of uniform distribution
	\begin{align*}
		P(x)&=k && a<x<b\\
		&=0 && \text{otherwise}
	\end{align*}
\end{theorem}



\begin{itemize}
	\item $ \int_a^b f(x)\, dx=\int_a^b k\, dx = k[x]_a^b=k(b-a)=1$, therefore $k= \frac{1}{b-a} $
	\item We denote it as, $ X \sim U(a,b) $
	\item $ f(x)=\frac{1}{b-a} $\\ 
\end{itemize}

\includegraphics[scale=0.3]{uniform}

\subsection{CDF of uniform distribution}
\begin{theorem}\label{ref:cdfuniform}
	CDF of uniform distribution
	\begin{align*}
		F(x)&=0 && x \leq a\\
		&= P(X\leq x)= \int_a^x f(x)\,dx=\frac{x-a}{b-a} && a<x<b\\
			&=1 && x\geq b
	\end{align*}
\end{theorem}


\subsection{Expectation and variance of uniform distribution}
\begin{theorem}
	Expected value of $ X \sim U(a,b) $ is equal to $\frac{(a+b)}{2}$
\end{theorem}


\begin{proof}
	Consider the expectation of the uniform distribution as,
	\begin{align*}
		E[x]&=\int_a^b x P(x)\, dx\\
		&= \int_a^b x \frac{1}{b-a}\, dx\\
		&= \frac{1}{b-a} \int_a^b x\, dx\\
		&= \frac{a+b}{2}
	\end{align*}
\end{proof}

\begin{theorem}
	Variance of uniform distribution is equal to $ \frac{1}{12} (b-a)^2$
\end{theorem}
\begin{proof}
	We begin by finding out $ E[X^2] $
	\begin{align*}
		E[X^2]&=\int_a^b x^2 P(x)\, dx\\
		&= \int_a^b x^2 \frac{1}{b-a}\, dx\\
		&= \frac{1}{b-a}\int_a^b x^2\, dx\\
		&= \frac{1}{3}\left( a^2+ab+b^2\right)
\intertext{Now we can find the variance as $ V[X]=E[X^2]-E[X]^2 $ as follows,}
		V[X]&=E[X^2]-E[X]^2\\
		&= \frac{1}{3}\left( a^2+ab+b^2\right)-\left( \frac{a+b}{2}\right)^2\\
		&=\frac{(b-a)^2}{12}
	\end{align*}
\end{proof}


\subsection{Raw moments of uniform distribution}
The $ r^{th} $ raw moment of the uniform distribution is given as 

\begin{align*}
	\mu_r'&=E[X^r]=\int_a^b x^r f(x)\, dx \\
	&= \frac{1}{b-a}\left[ \frac{x^{r+1}}{r+1} \right]_a^b=\frac{b^{r+1}-a^{r+1}}{(b-a)(r+1)}
\end{align*}


\begin{example}
	Suppose in a quiz there are 30 participants. A question is given to all 30 participants and the time allowed is 25 seconds. 
\end{example}
\begin{proof}
	Let $ X$ denote the time to respond.\\
	$ X \sim U(0,25) $, the pdf is given by 
	$ f(x) =\frac{1}{25}; 0<x<25$ and $ 0 $ otherwise.
	$$ P(x\leq 6)=\int_0^6 f(x)\, dx=\int_0^6 \frac{1}{25}\, dx = \frac{151}{25}$$
	\[P(6 \leq x \leq 10)=\int_6^10 f(x)\, dx = \int_6^{10} \frac{1}{25}\, dx= \frac{101}{25} \]
\end{proof}

\begin{example}
	A r.v. $ x $ is said to follow uniform dist with $ \mu=1 $ and $ V(x) = 4/3$. Obtain $ P(x<0) $.
\end{example}
\begin{proof}
	First begin by finding out the parameters for the unfirom distribution.\\
	First consider the mean,
	\begin{align*}
		\mu&=1\\
		\frac{a+b}{2}&=1\\
		a+b&=2
	\end{align*}
	Then consider the variance,
	\begin{align*}
		V(x)&=\frac{4}{3}\\
		\frac{(b-a)^2}{12}&=\frac{4}{3}\\
		(b-a)^2&=16
	\end{align*}
	Solving two simultaneous equations we get $ a=-1, b=3 $.\\
	Therefore, we have $ X \sim U(-1,3) $\\
	\begin{align*}
		P(x\leq0)&=F(0)=\frac{0+1}{4}=\frac{1}{4}
	\end{align*}
\end{proof}


\begin{example}
	If $ X \sim U(-3,3) $, find $ P(x<2) $, $ P(|x|<2) $, $ P(|x-2|<2),$ also obtain $ k $ if $ P(x>k) =1/3$
\end{example}
\begin{proof}
	\begin{align*}
		P(x<2)&=F(2)=\frac{2+3}{6}=
		\frac{5}{6}\\
		P(|x|<2)&=\int_-2^2 \frac{1}{6}\, dx = \frac{2}{3}\\
		P(|x-2|<2)&=\int_0^3 \frac{1}{6}=\frac{1}{2}\\
		P(x>k)=1/3\implies \dots
	\end{align*}
	Complete this l8r alig8r
	
\end{proof}

\subsection{MGF of Uniform distribution}
\begin{theorem}
	MGF of Uniform distribution  $= \frac{e^{bt}-e^{at}}{t(b-a)}, t\neq0 $ and $ =1, t=0 $
\end{theorem}
\begin{proof}
	\begin{align*}
		M_x(t)&=E[e^{tx}]=\int_a^b \frac{e^{tx}}{b-a}\, dt= \frac{e^{bt}-e^{at}}{(b-a)t}
		\intertext{The Taylor series for this can be expressed as the following,}
		M_x(t)&=\frac{b-a}{b-a}+\frac{b^2-a^2}{2(b-a)}t+\frac{b^3-a^3}{3(b-a)}\frac{t^2}{2!}+\cdots
	\end{align*}
	Therefore we can say, 
	\begin{align*}
		 \mu_1'&= \text{coeff of } t=\frac{b^2-a^2}{2(b-a)}=\frac{a+b}{2}\\
		 \mu_2'&= \text{coeff of }\frac{t^2}{2!}=\frac{b^3-a^3}{3(b-a)}
	\end{align*}
And we can say $ \mu_2 = \dots$ 
\end{proof}

\subsection{Applications of uniform distribution}
\begin{enumerate}
	\item Assumption of uniform death for insurance\\
    $ \vdots $
\end{enumerate}
Write sumthin here

\section{Gamma distribution}
\begin{definition}[Gamma distribution]
	A r.v. $ `X' $ is said to follow gamma distribution $ X \sim G(\lambda, \theta) $. Where $ \lambda=$ shape parameter and $ \theta= $ scale parameter.
\end{definition}

\subsection{PDF of Gamma distribution}
\begin{definition}[PDF of Gamma distribution]
	\begin{align*}
		f(x,\lambda, \theta)&=\frac{\theta^{\lambda}}{\Gamma (\lambda)} e^{-\theta x} x^{\lambda -1}&& x>0, \lambda>0, \theta>0\\
		&= 0 && \text{otherwise}
	\end{align*}
Where $ \Gamma (\lambda)=(\lambda-1)!=(\lambda-1)\Gamma (\lambda -1) $.\\
\end{definition}





\begin{corollary}
	If $ \theta=1 $ we will have gamma distribution with a single parameter $ \lambda $ which is called the standard gamma distribution.
	\begin{align*}
		X\sim G(\lambda)&= \frac{e^{-x} x^{\lambda -1}}{\Gamma (\lambda )} && x>0, \lambda >0\\
		&=0 && \text{otherwise}
	\end{align*}
\end{corollary}

\begin{corollary}
	If $ \lambda=1, X \sim G(1, \theta) = \text{Exp}(\theta)$.
\end{corollary}

\begin{corollary}
	If $ \lambda=1, \theta=1 $, $ X \sim  $ Standard exponential distribution, i.e.
	\begin{align*}
		f(x)&= e^{-x} && x>0\\
		&=0 && \text{otherwise}
	\end{align*}
\end{corollary}


\begin{definition}[Gamma function]
	\[ \Gamma(\lambda) =\int_0^\infty e^{-x} x^{\lambda -1 }\, dx \]
\end{definition}

\begin{definition}[Gamma integral]
	\[ \int_0^\infty e^{- \theta x} x^{\lambda -1}\, dx = \frac{\Gamma (\lambda)}{\theta^\lambda}\]
\end{definition}

\subsection{CDF of Gamma distribution}
\begin{theorem}
 	CDF of Gamma distribution is given as \[ F(x) = \]
\end{theorem}
\begin{proof}
	\begin{align*}
		F(x)&=P(X<x)=\int_0^x \frac{\theta ^\lambda e^{- \theta x} x^{\lambda -1}}{\Gamma (\lambda )}\, dx\\
		&=\frac{\theta^{\lambda}}{\Gamma(\lambda)} \int_0^x x^{\lambda-1} e^{-\theta x}\, dx
	\end{align*}
\end{proof}

\subsection{Raw moments of Gamma distribution}
\begin{theorem}
	The $ r^{th} $ aw moment of the Gamma distribution is given by \[\mu_r'=\frac{\Gamma(\lambda+1)}{\Gamma(\lambda)\theta^r} \]
\end{theorem}
\begin{proof}
	\begin{align*}
		\mu_r'=E[x^r]&=\int_0^\infty \frac{x^r e^{-\theta x} x^{\lambda -1}}{\Gamma(\lambda)}\, dx\\
		&=\int_0^{\infty} \dfrac{\theta^{\lambda} e^{-\theta x}x^{\lambda+r-1}  }{\Gamma(\lambda)}\, dx\\
		&= \frac{\Gamma(\lambda+1)}{\Gamma(\lambda) \theta^r}
	\end{align*}
\end{proof}

\subsection{Mean and Variance of Gamma distribution}
Now we can find $ \mu_1', \mu_2' $
\begin{align*}
	E[x]= \mu_1'&=\frac{\lambda}{\theta}\\
	\mu_2'&= \frac{\lambda(\lambda+1)}{\theta^2}\\
	V[x] = \mu_2 &=\mu_2'-\mu_1'^2=\frac{\lambda(\lambda +1)}{\theta^2}-\frac{\lambda^2}{\theta^2}=\frac{\lambda}{\theta^2}
\end{align*}

\subsection{MGF of Gamma distribution}
\begin{align*}
	 E[e^{tx}]&=\int_0^{\infty} e^{tx} \frac{\theta^\lambda e^{-\theta x} x^{\lambda -1}}{\Gamma(\lambda )}\, dx\\
	 &= \frac{\theta^{\lambda}}{\Gamma (\lambda)} \int_0^{\infty} e^{-(\theta - t) x}x^{\lambda -1}\, dx\\
	 &= \frac{\theta^\lambda}{\Gamma(\lambda )} \frac{\Gamma(\lambda )}{(\theta -t )^{\lambda}}=\left( \frac{\theta}{\theta - t}\right)^\lambda\\
	 &=\left(1- \frac{t}{\theta}\right)^{- \lambda}
\end{align*}

\subsection{CGF of Gamma distribution}
\begin{align*}
	K_x(t)&=\log \left( 1- \frac{t}{\theta} \right)^{-\lambda}\\
	&= -\lambda \log \left(1- \frac{t}{\theta}\right)\\
	&= \frac{\lambda t }{\theta }+\frac{\lambda t^2}{2 \theta^2}+\frac{\lambda t^3}{3 \theta^3}+ \cdots
\end{align*}
Using this we can get the mean and variance easily.
\begin{align*}
	\text{Mean }&=k_1=\frac{\lambda }{\theta}\\
	\text{Variance }&= k_2= \frac{\lambda }{\theta^2}
	\end{align*}

\subsection{Additive property of Gamma distribution}
If $ X_i (i=1,\dots, k) $ are $ k $ independent Gamma distributions with parametrs $ \lambda_1, \lambda_2, \dots, \lambda_k  $ and $ \theta  $ respectively, then,
\begin{align*}
	\sum_{i=1}^{k}X_i &\sim G(\sum_{i=1}^{k} \lambda_i, \theta)\\
	M_{X_i}(t)&= \left(1 -\frac{t}{\theta} \right)^{-\lambda_i}
	\intertext{Let $ Z=\sum X_i $}
	M_Z(t)&= \prod_{i=1}^k \left(1- \frac{t}{\theta}\right)^{-\lambda_i}\\
	&= \left(1- \frac{t}{\theta}\right)^{- \sum \lambda_i}
	\intertext{By uniqueness property of mgf}
	\sum_i X_i &\sim G \left(\sum_i \lambda_i, \theta \right)
\end{align*}

\subsection{Limiting form of Gamma distribution}
\begin{align*}
	\beta_1 &= \frac{4}{\lambda}, \text{ as $ \lambda \rightarrow \infty $}, \beta_1 \rightarrow 0 \implies \text{Normal dist}\\
	\beta_2 &= 3+ \frac{6}{\lambda } \text{ as $ \lambda \rightarrow \infty $}, \beta_2 \rightarrow 3 \implies \text{Normal dist}
\end{align*}
Note that they are both independent of $ \theta  $.\\
Therefore, as $ \lambda \rightarrow \infty  $ we have $ G(\lambda , \infty ) \rightarrow N \left(\frac{\lambda}{\theta}, \frac{\lambda }{\theta^2}\right)$.

\subsection{Applications of Gamma distribution}
Idk write something bruh

\section{Exponential distribution}
\subsection{PDF of Exponential Distribution}
\begin{definition}[PDF of Exponential distribution]
	A r.v. $ x $ os said to follow the exponential distribution with parameter $ \theta  $ if its pdf is given by 
	\begin{align*}
		f(x)&= \theta e^{-\theta x} && x\geq 0, \theta > 0\\
		&= 0 && \text{otherwise}
	\end{align*}
\end{definition}


\subsection{INCOMPLETE CDF of exponential distribution}
\begin{align*}
	F[x]=1-e^{-\theta x}
\end{align*}
\textbf{FILL THIS UP}
\subsection{Raw moment of exponential distribution}

\begin{theorem}
	The $ r^{th} $ raw moment for exponential distribution is given by \[ \mu_r'=\frac{r!}{\theta^r} \]
\end{theorem}
\begin{proof}
	\begin{align*}
		\mu_r'=E[x^r]&=\int_0^\infty x^r \theta e^{-\theta x}\, dx\\
		&=\frac{\Gamma(r+1)}{\theta^r}\\
		&= \frac{r!}{\theta^r}
	\end{align*}
\end{proof}

\subsection{Mean and variance of exponential distribution}
\begin{theorem}
	The mean of exponential distribution is given be \[ \mu=\frac{1}{\theta} \]
\end{theorem}
\begin{proof}
	Consider $ r=1 $,
	\begin{align*}
		\mu_1'=\frac{1}{\theta}
	\end{align*}
\end{proof}

\begin{theorem}
	The variance of the exponential distribution is given by \[ \mu_2 = \frac{1}{\theta^2}\]
\end{theorem}
\begin{proof}
	First find $ \mu_2' $
	\begin{align*}
		\mu_2'=\frac{2}{\theta^2}
	\end{align*}
So now we can compute the variance as $ \frac{1}{\theta^2} $
\end{proof}

\subsection{MGF of exponential distribution}
\begin{theorem}
	MGF of exponential distribution is given by \[ M_x(t)= \left(1- \frac{t}{\theta}\right)^{-1} \]
\end{theorem}
\begin{proof}
	\begin{align*}
		M_x(t)&=E[e^{tx}]\\
		&=\int_0^\infty e^{tx} \theta e^{-\theta x}\, dx\\
		&=\theta \int_0^\infty e^{x(t-\theta)} x^{1-1}\, dx\\
		&= \frac{\theta \Gamma(1)}{\theta - t}\\
		&=\frac{\theta }{\theta-t}\\
		&= \left(1- \frac{t}{\theta}\right)^{-1}
	\end{align*}
\end{proof}

\subsection{CGF of exponential distribution}
\begin{theorem}
	CGF of exponential distribution is given by \[ K_x(t)=-\log\left(1- \frac{t}{\theta}\right) \]
\end{theorem}
\begin{proof}
	\begin{align*}
		K_x(t)&= \log \left(1-\frac{t}{\theta}\right)^{-1}\\
		&= -\log\left(1- \frac{t}{\theta}\right)\\
		&= \frac{t}{\theta }+\frac{t^2}{2 \theta^2}+\frac{t^3}{3 \theta^3}
	\end{align*}
We can say the general $ r^{th} $ cumulant is given by $ K_r=\frac{(r-1)!}{\theta^r} $
\end{proof}

\subsection{Additive property of exponential variates}
\begin{theorem}
	If $ x_1,x_2,\dots,x_k $ are $ k $ independent exponential variates each with parameter $ \theta  $then \[ \sum_{i=1}^k x_i \sim G(k, \theta)\]
\end{theorem}
\begin{proof}
	We will do this with the MGF. Consider taht $ Z=\sum{i=1}^{k} x_i $.
	\begin{align*}
		M_z(t)&=\prod_{i=1}^{k} M_x(t)\\
		&= \prod_{i=1}^k \left(1- \frac{t}{\theta}\right)^{-1}\\
		&= \left(1- \frac{t}{\theta}\right)^{-k}
	\end{align*}
Therefore, (by uniqueness property of MGF) comparing this MGF to that of the gamma distribution we can say that,
$$ \sum_{i=1}^k x_i = Z \sim G(k,\theta)$$
\end{proof}

\subsection{Lack of memory of exponential distribution}
\begin{theorem}
	For a exponentially distributed random variate, $ P[x>a+b \mid x>a]=P[x>b] $
\end{theorem}
\begin{proof}
	Let $ X \sim E(\theta) $. Consider first case
	\begin{align*}
		P[x>a+b \mid x>a]&= \frac{P[x>a+b]}{P[x>a]}\\
		&=\frac{\int_{a+b}^\infty \theta e^{-\theta x}\, dx}{\int_a^\infty \theta e^{-\theta x}\, dx}\\
		&=\frac{e^{-\theta a+b}}{e^{-\theta a}}\\
		&=e^{-\theta b}
	\end{align*}
	Consider second case now,
	\begin{align*}
		P[x>b]&=\int_b^{\infty }\theta e^{-\theta x}\, dx=e^{-\theta b}
	\end{align*}
	Equality holds.
\end{proof}

\section{INCOMPLETE Laplace distribution (Double exponential)}
\subsection{PDF}
\begin{definition}[PDF of Laplace distribution]
	$ X\sim L (\lambda, \mu)$\[ f(x)=\begin{cases}
		\frac{1}{2 \lambda } e^{-\left|\frac{x-\mu }{\lambda}\right|} & -\infty<x<\infty\\
		0 & \text{otherwise}
	\end{cases} \]
\end{definition}
\subsection{CDF}
\begin{definition}[CDF of Laplace distribution]
	\[ F[x]=\begin{cases}
		content...
	\end{cases} \]
\end{definition}

\subsection{Raw moment}
\begin{theorem}
	The $ r^{th} $ raw moment for the Laplace distribution is given by \[ \mu_r'= \]
\end{theorem}
\begin{proof}
	\begin{align*}
		\mu_r'&=E[x^r]=\int_{-\infty}^\infty \frac{x^r}{2 \lambda }e^{-\left|\frac{x-\mu }{\lambda}\, dx\right|}
		\intertext{Transform $ (x-\mu)/\lambda =z $}
		&=\frac{1}{2\lambda }\left( \int_{-\infty}^\infty (z\lambda+\mu)^r e^{-|z|}\, \lambda\, dz  \right)\\
		&=\frac{1}{2} \left(\int_{-\infty}^\infty \sum_{k=0}^{r}\binom{r}{k} (z-\lambda)^k \mu^{r-k} e^{-|z| \, dz}\right)\\
		&=\frac{1}{2}\sum_{k=0}^r \left[\binom{r}{k}\lambda^k \mu^{r-k} \int_{-\infty}^\infty z^k e^{-|z|}\, dz \right]\\
		\intertext{Complete this up}
		&=\frac{1}{2}\sum_{k=0}^{r} \left[\binom{r}{k} \lambda^k \mu^{r-k} k! (1+(-1)^k)\right]
	\end{align*}
\end{proof}

\subsection{Mean and variance}
We can do this with the raw moments above but instead we will do it with the PDF.
\begin{theorem}
	Expectation of laplace distribution is given as \[ E[x]= \]
\end{theorem}
\begin{proof}
	\begin{align*}
		E[x]&=\int_{-\infty }^{\infty }x f(x)\, dx\\
		&=\int_{-\infty }^{\infty }\frac{x}{2\lambda}e^{-\left|\frac{x-\mu }{\lambda}\right|}\, dx
		\intertext{Split it around $ \mu $}
		&=\frac{1}{2\lambda}\left( \int_{-\infty}^\mu x e^{\frac{x-\mu }{\lambda}}\, dx + \int_\mu^{\infty}x e^{-\frac{x-\mu }{\lambda}}\, dx \right)\\
		&= \frac{1}{2\lambda} \left[ e^{-\mu/ \lambda}\int_{-\infty}^{\mu} xe^{x/\lambda}\, dx + e^{\mu/\lambda}\int_{\mu}^{\infty}xe^{-x/\lambda}\, dz\right]\\
		&=\frac{1}{2 \lambda} \left[e^{-\mu/\lambda } \lambda(x-\lambda)e^{x/ \lambda}-e^{\mu/\lambda }(\lambda(x+\lambda)e^{-x/ \lambda}) \right]\\
		&=\mu
	\end{align*}
\end{proof}

\begin{theorem}
	Expectation of $ x^2 $ in Laplace distribution is given be \[ E[x^2]=bruh \]
\end{theorem}
\begin{proof}
	\begin{align*}
		E[x^2]&=\int_{-\infty}^\infty x^2 \frac{1}{2\lambda }e^{-\left| \frac{x-\mu }{\lambda} \right|}
		\intertext{Split it around $ \mu $}
		&=\frac{1}{2\lambda}\left( \int_{-\infty}^\mu x^2 e^{\frac{x-\mu }{\lambda}}\, dx + \int_\mu^{\infty}x^2 e^{-\frac{x-\mu }{\lambda}}\, dx \right)\\
		&= \frac{1}{2\lambda} \left[ e^{-\mu/ \lambda}\int_{-\infty}^{\mu} x^2e^{x/\lambda}\, dx + e^{\mu/\lambda}\int_{\mu}^{\infty}x^2e^{-x/\lambda}\, dx\right]\\
		&= \frac{1}{2\lambda} \left[ e^{-\mu/ \lambda}(\lambda (x^2-2\lambda x+2\lambda^2)e^{x/\lambda}) - e^{\mu/\lambda}(\lambda(x^2+2\lambda x+2\lambda^2)e^{-x/ \lambda})\right]\\
		&=2\lambda^2
	\end{align*}
\end{proof}
\begin{theorem}
	Variance of Laplace distribution is given as \[ V[x]= \]
\end{theorem}


\subsection{MGF}
\begin{theorem}
	MGF of the Laplace distribution is given by \[ M_x(t)=bruh \]
\end{theorem}
\begin{proof}
	\begin{align*}
		M_x(t)&=E[e^{tx}]=\int_{-\infty}^\infty \frac{1}{2\lambda} e^{tx-\left|\frac{x-\mu }{\lambda}\right|}\\
		&=\frac{1}{2\lambda}\left[e^{-\mu/\lambda} \int_{-\infty}^\mu e^{x\left(t+\frac{1}{\lambda} \right)}\, dx + e^{\mu/\lambda}\int_\mu^{\infty} e^{-x\left(\frac{1}{\lambda}-t \right)}\, dx\right]\\
		&= \frac{1}{2 \lambda } \left[e^{-\mu/\lambda} \left(\frac{e^{\mu \left( \frac{1}{\lambda} +t \right)}}{\frac{1}{\lambda}+t}\right) + e^{\mu/\lambda} \left(\frac{-e^{\mu \left(\frac{1}{\lambda}-t \right)}}{-\frac{1}{\lambda} +t}\right)\right]\\
		&=\frac{1}{2\lambda }\left[\frac{e^{\mu t}}{t+\frac{1}{\lambda}}-\frac{e^{\mu t}}{t-\frac{1}{\lambda}}\right]
	\end{align*}
\end{proof}

Plot a graph for the beta-1 dsitribution when alpha=5, beta=2
\subsection{CGF}

\section{Beta distribution of Type-I}
\subsection{PDF}
\begin{definition}[PDF of Beta I]
	\[ f(x)=\begin{cases}
		\frac{1}{\beta(m,n)}x^{m-1}(1-x)^{n-1} & 0<x<1; m,n>0\\
		0 & \text{otherwise}
	\end{cases} \]
	Where $ \beta(m,n) = \frac{\Gamma(m)\Gamma(n)}{\Gamma(m+n)}$
\end{definition}
Note the following,
\begin{enumerate}
	\item We can say, $ X \sim \beta_1 (m,n) $ where $ m,n $ are the parameters of the distribution.
	\item Since $ f(x) $ is a pdf we have the following,
	\begin{align*}
		\int_0^1 f(x)\, dx &= \int_0^1 \frac{1}{\beta(m,n)} x^{m-1} (1-x)^{n-1}\, dx\\
		&= \int_0^1 
	\end{align*}
\end{enumerate}

\subsection{Raw moments}
\begin{theorem}
	The $ r^{th}$ raw moment of the Beta I distribution is given by \[ \mu_r'=\frac{\Gamma(m+n)\Gamma(r+m)}{\Gamma(m)\Gamma(m+n+r)} \]
\end{theorem}
\begin{proof}
	\begin{align*}
		\mu_{r}'=E[x^r]&=\int_0^1 \frac{1}{\beta(m,n)} x^{r+m-1}(1-x)^{n-1}\, dx\\
		&=\frac{\Gamma(m+n)\Gamma(r+m)}{\Gamma(m)\Gamma(m+n+r)}
	\end{align*}
\end{proof}

\subsection{Mean and Variance}
\begin{theorem}
	Mean of Beta I distribution is given by \[ E[x]=\frac{m}{m+n} \]
\end{theorem}
\begin{proof}
	\begin{align*}
		E[x]=\mu_1'=\frac{\Gamma(m+n)\Gamma(m+1)}{\Gamma(m)+\Gamma(m+n+1)}=\frac{m}{m+n}
	\end{align*}
\end{proof}

\begin{theorem}
	Variance of Beta I distribution is given by \[ V[x]=\frac{mn}{(m+n)^2(m+n+1)} \]
\end{theorem}
\begin{proof}
	\begin{align*}
		\mu_2'&=\frac{(m+1)(m)}{(m+n)(m+n+1)}
	\end{align*}
So now we have the variance given as,
\begin{align*}
	\mu_2&=\mu_2'-\mu_1'^2\\
	&=\frac{mn}{(m+n)^2(m+n+1)}
\end{align*}
\end{proof}

\section{Beta distribution of Type-II}
\subsection{PDF}
\begin{definition}[PDF of Beta-II distribution]
	\[ f(x)=\begin{cases}
		\frac{1}{\beta(m,n)}\frac{x^{m-1}}{(1+x)^{m+n}} & 0<x<\infty; m,n>0\\
		0 & \text{otherwise}
	\end{cases} \]
\end{definition}
Note the following,
\begin{enumerate}
	\item $ X $ is said to follow $ \beta_2(m,n) $ as $ X \sim \beta_2(m,n) $
	\item \begin{align*}
		\int_0^{\infty }f(x)\, dx&=\int_0^\infty \frac{x^{m+1}}{(1+x)^{m+n}}=\beta(m,n)
	\end{align*}
\end{enumerate}

\subsection{Raw moments}
\begin{theorem}[Raw moments of Beta-2 distribution]
	The raw moments of the Beta-2 distribution is given by \[ \mu_r'= \frac{\Gamma(m+r)\Gamma(n-r)}{\Gamma(m)\Gamma(n)} \]
\end{theorem}
\begin{proof}
	\begin{align*}
		\mu_r'&=E[x^r]=\int_0^{\infty} \frac{1}{\beta(m,n)}\frac{x^{m+r-1}}{(1+x)^{m+n}}\, dx\\
		&=\frac{\Gamma(m+r)\Gamma(n-r)}{\Gamma(m)\Gamma(n)}
	\end{align*}
\end{proof}

\subsection{Mean and variance}
\begin{theorem}[Mean of Beta-2 distribution]
	The mean of Beta-2 distribution is given by \[ E[x]=\frac{m}{n-1} \]
\end{theorem}
\begin{proof}
	\begin{align*}
		E[x]&=\mu_1'=\frac{\Gamma(m+1)\Gamma(n-1)}{\Gamma(m)\Gamma(n)}\\
		&=\frac{m}{n-1}
	\end{align*}
\end{proof}

\begin{theorem}[Variance of Beta-2 distribution]
	The variance of Beta-2 distribution is given by \[ V[x]=\frac{m(m+n-1)}{(n-2)(n-1)^2} \]
\end{theorem}
\begin{proof}
	First consider the 2nd raw moment,
	\begin{align*}
		\mu_2'&=\frac{m(m+1)}{(n-2)(n-2)}
	\end{align*}
	Now we can compute the variance as follows
	\begin{align*}
		V[x]&=\mu_2=\mu_2'-\mu_1'^2=\frac{m(m+n-1)}{(n-2)(n-1)^2}
	\end{align*}
\end{proof}



\chapter{Chi-square distribution}



\chapter{F-distribution}


\backmatter


\thispagestyle{empty}%If your book ends with the even numbered page, copy and paste it twice. With the odd numbered page, do it three times.
{\ }
\newpage

\end{document}
